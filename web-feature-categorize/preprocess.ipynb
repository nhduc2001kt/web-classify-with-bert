{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attr):\n",
    "        global start_tag\n",
    "        global temp_links\n",
    "        global domain\n",
    "        start_tag = tag\n",
    "        if tag == 'a':\n",
    "            for item in attr:\n",
    "                # Get links\n",
    "                if item[0] == 'href':\n",
    "                    # href=\"/abc\"\n",
    "                    if item[1].startswith(\"/\"):\n",
    "                        temp_links += ['https://' + domain + item[1]]\n",
    "                    # href=\"domain/abc\"\n",
    "                    elif item[1].startswith(domain):\n",
    "                        temp_links += [item[1]]\n",
    "        temp_links = list(set(temp_links))\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        global start_tag\n",
    "        global all_data\n",
    "        if start_tag != 'script' and start_tag != 'style':\n",
    "            data = data.rstrip().lstrip()\n",
    "            if len(data) > 0:\n",
    "                all_data += [data]\n",
    "\n",
    "\n",
    "# Dictionary to save parsed robots.txt files\n",
    "ROBOTS = dict()\n",
    "\n",
    "\n",
    "def robot_check(url):\n",
    "    netloc = urlparse(url).netloc\n",
    "    if netloc not in ROBOTS:\n",
    "        robotsurl = urljoin(url, '/robots.txt')\n",
    "        ROBOTS[netloc] = RobotFileParser(robotsurl)\n",
    "        try:\n",
    "            ROBOTS[netloc].read()\n",
    "        except:\n",
    "            print(netloc)\n",
    "    return ROBOTS[netloc].can_fetch('*', url)\n",
    "\n",
    "\n",
    "def get_page(url):\n",
    "    if robot_check(url):\n",
    "        try:\n",
    "            fd = urlopen(url)\n",
    "            content = fd.read()\n",
    "            fd.close()\n",
    "            return content.decode('utf8')\n",
    "        except:\n",
    "            return ''\n",
    "    else:\n",
    "        print(\"Can't fetch data from \" + url)\n",
    "        return ''\n",
    "\n",
    "\n",
    "def deep(d=2):\n",
    "    global parser\n",
    "    global all_links\n",
    "    global temp_links\n",
    "    global url_limit\n",
    "    global domain\n",
    "    for i in range(d):\n",
    "        all_links = copy.deepcopy(temp_links)\n",
    "        temp_links = []\n",
    "        print(\"Deep:\", i + 1)\n",
    "        print(len(all_links))\n",
    "        if url_limit > 0:\n",
    "            for l in all_links:\n",
    "                url_limit -= 1\n",
    "                # print(url_limit)\n",
    "                if url_limit <= 0:\n",
    "                    break\n",
    "                str_raw = get_page(l)\n",
    "                if isinstance(str_raw, str) or str == '':\n",
    "                    parser.feed(str_raw)\n",
    "        print(\"End deep:\", i + 1)\n",
    "\n",
    "\n",
    "# URL = 'https://www.bbc.co.uk/iplayer'\n",
    "start_tag = ''\n",
    "all_links = []\n",
    "temp_links = []\n",
    "all_data = []\n",
    "url_limit = 50\n",
    "parser = MyHTMLParser()\n",
    "domain = ''\n",
    "input_data = []\n",
    "\n",
    "\n",
    "def run(URL):\n",
    "    global parser\n",
    "    global domain\n",
    "    global input_data\n",
    "    global all_data\n",
    "    domain = urlparse(URL).netloc\n",
    "    parser.feed(get_page(URL))\n",
    "    deep()\n",
    "    # for d in all_data:\n",
    "    #   input_data += d.split(' ')\n",
    "    input_data = ' '.join(list(all_data))\n",
    "    return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['url', 'category']\n",
    "data = pd.read_csv('urls.csv', usecols=usecols)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = ''\n",
    "data.to_csv('dataset.csv', index=False)\n",
    "for i, v in data.loc[:, 'url'].items():\n",
    "    print(\"########### Fetch link\", i, \":\", v, \"###########\")\n",
    "    try:\n",
    "        text = run(v)\n",
    "        data.at[i, 'text'] = text\n",
    "    except:\n",
    "        data.at[i, 'text'] = 'ERROR'\n",
    "    data.to_csv('dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratiocoffee.com\n",
      "Can't fetch data from https://ratiocoffee.com/\n",
      "Deep: 1\n",
      "0\n",
      "End deep: 1\n",
      "Deep: 2\n",
      "0\n",
      "End deep: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(run('https://ratiocoffee.com/'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
